# Data Engineering Pipeline

A comprehensive data engineering solution that ingests data from multiple sources, stores it in an AWS S3 data lake, transforms and standardizes it using Apache Spark, and produces a curated star-schema dataset for business intelligence and analytics.

## ğŸ—ï¸ Architecture Overview

This project implements a complete end-to-end data pipeline with the following components:

```
Data Sources â†’ Ingestion â†’ S3 Data Lake â†’ Transformation â†’ Star Schema â†’ BI Tools
                              (Raw)      (Spark)      (Curated)   (Power BI)
                                â†“
                          Data Masking
```

### Key Features

- **Multi-Source Data Ingestion**: Support for databases (PostgreSQL, MySQL), REST APIs, and file sources (CSV, JSON, Parquet)
- **AWS S3 Data Lake**: Three-tier data lake architecture (Raw, Processed, Curated)
- **Apache Spark Transformation**: Scalable data transformation and standardization
- **Star Schema Data Warehouse**: Optimized dimensional modeling for analytics
- **Data Masking**: PII protection with multiple masking strategies (hashing, tokenization, pattern masking)
- **Apache Airflow Orchestration**: Automated scheduling and workflow management
- **BI Tool Integration**: Ready for Power BI, Tableau, and other analytics platforms

## ğŸ“ Project Structure

```
engineering-proj/
â”œâ”€â”€ data_pipeline/
â”‚   â”œâ”€â”€ ingestion/          # Data ingestion modules
â”‚   â”‚   â”œâ”€â”€ base.py         # Base ingestion classes
â”‚   â”‚   â”œâ”€â”€ database_source.py  # Database connectors
â”‚   â”‚   â”œâ”€â”€ api_source.py   # API data source
â”‚   â”‚   â””â”€â”€ file_source.py  # File-based sources
â”‚   â”œâ”€â”€ transformation/     # Spark transformation modules
â”‚   â”‚   â”œâ”€â”€ spark_transformer.py  # Base transformer
â”‚   â”‚   â””â”€â”€ star_schema.py  # Star schema builder
â”‚   â”œâ”€â”€ masking/           # Data masking
â”‚   â”‚   â””â”€â”€ data_masker.py  # Masking utilities
â”‚   â””â”€â”€ utils/             # Utility modules
â”‚       â”œâ”€â”€ s3_utils.py     # S3 operations
â”‚       â””â”€â”€ config_loader.py # Configuration management
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ data_pipeline_dag.py  # Airflow orchestration
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline_config.yaml  # Pipeline configuration
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_star_schema.sql   # DDL for star schema
â”‚   â””â”€â”€ analytical_queries.sql   # Sample queries
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_pipeline.py        # Setup script
â”‚   â””â”€â”€ generate_sample_data.py  # Sample data generator
â”œâ”€â”€ data/                  # Local data storage
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ curated/
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env.example          # Environment variables template
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Apache Spark 3.4+
- Apache Airflow 2.7+
- AWS Account with S3 access
- PostgreSQL/MySQL (optional, for database sources)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/lamle3105/engineering-proj.git
   cd engineering-proj
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your AWS credentials and configuration
   ```

5. **Setup the data pipeline**
   ```bash
   python scripts/setup_pipeline.py
   ```

### Configuration

Edit `config/pipeline_config.yaml` to configure:
- Data sources (databases, APIs, files)
- S3 bucket and data lake structure
- Spark settings
- Data masking rules
- Airflow scheduling

Edit `.env` file with your credentials:
```env
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
S3_BUCKET_NAME=your-data-lake-bucket
```

## ğŸ“Š Data Pipeline Components

### 1. Data Ingestion

The pipeline supports multiple data sources:

**Database Sources**
```python
from data_pipeline.ingestion.database_source import DatabaseSource

config = {
    'name': 'sales_db',
    'type': 'postgresql',
    'connection': {
        'host': 'localhost',
        'port': 5432,
        'database': 'sales',
        'user': 'admin',
        'password': 'password'
    },
    'tables': ['sales_transactions', 'customers']
}

source = DatabaseSource(config)
```

**API Sources**
```python
from data_pipeline.ingestion.api_source import APISource

config = {
    'name': 'inventory_api',
    'type': 'api',
    'endpoint': 'https://api.example.com/inventory',
    'auth_type': 'bearer',
    'api_key': 'your_api_key'
}

source = APISource(config)
```

### 2. Data Transformation with Spark

```python
from data_pipeline.transformation.spark_transformer import DataStandardizer

spark_config = {
    'app_name': 'DataPipeline',
    'master': 'local[*]'
}

standardizer = DataStandardizer(spark_config)
df = standardizer.read_from_s3('s3://bucket/raw/')
standardized_df = standardizer.standardize(df)
```

### 3. Star Schema

The pipeline creates a dimensional model with:
- **Fact Table**: `fact_sales`
- **Dimension Tables**: 
  - `dim_customer`
  - `dim_product`
  - `dim_date`
  - `dim_location`

### 4. Data Masking

Configure masking in `pipeline_config.yaml`:

```yaml
data_masking:
  enabled: true
  fields:
    email:
      method: hash
      algorithm: sha256
    phone:
      method: mask
      pattern: "XXX-XXX-####"
    ssn:
      method: mask
      pattern: "XXX-XX-####"
    credit_card:
      method: tokenize
```

## ğŸ”„ Running the Pipeline

### Manual Execution

1. **Generate sample data** (optional)
   ```bash
   python scripts/generate_sample_data.py
   ```

2. **Run the pipeline** (via individual components or full DAG)

### Airflow Orchestration

1. **Initialize Airflow**
   ```bash
   export AIRFLOW_HOME=$(pwd)/airflow
   airflow db init
   ```

2. **Start Airflow**
   ```bash
   airflow webserver --port 8080
   airflow scheduler
   ```

3. **Access Airflow UI**
   - Navigate to http://localhost:8080
   - Enable the `data_pipeline_dag`
   - The DAG runs daily at 2 AM by default

### DAG Workflow

```
Start â†’ Ingest Data â†’ Transform & Standardize â†’ Apply Data Masking â†’ Build Star Schema â†’ Data Quality Checks â†’ End
```

## ğŸ“ˆ Business Intelligence Integration

### Power BI Connection

1. Use the Direct Query or Import mode
2. Connect to your data warehouse (PostgreSQL/MySQL)
3. Use the views created in `sql/create_star_schema.sql`
4. Import the star schema tables

### Sample Queries

See `sql/analytical_queries.sql` for example queries:
- Sales by product category
- Monthly sales trends
- Top customers by revenue
- Regional performance analysis

## ğŸ”’ Data Security

### Data Masking Features

- **Hashing**: SHA-256/MD5 for irreversible masking
- **Pattern Masking**: Show partial data (e.g., last 4 digits of phone)
- **Tokenization**: Replace sensitive values with tokens
- **Email Masking**: Mask username while keeping domain

### Best Practices

- Store credentials in `.env` (never commit to git)
- Use IAM roles for AWS access when possible
- Enable S3 encryption at rest
- Implement row-level security in your BI tool
- Regularly audit access logs

## ğŸ§ª Testing

Generate sample data for testing:
```bash
python scripts/generate_sample_data.py
```

This creates:
- 1,000 customer records
- 200 product records
- 50 location records
- 10,000 transaction records

## ğŸ“ Data Lake Structure

```
s3://your-bucket/
â”œâ”€â”€ raw/                 # Raw ingested data (90 days retention)
â”‚   â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ api/
â”‚   â””â”€â”€ files/
â”œâ”€â”€ processed/          # Standardized data (180 days retention)
â”‚   â””â”€â”€ masked/        # Masked sensitive data
â””â”€â”€ curated/           # Star schema tables (365 days retention)
    â”œâ”€â”€ dim_customer/
    â”œâ”€â”€ dim_product/
    â”œâ”€â”€ dim_date/
    â”œâ”€â”€ dim_location/
    â””â”€â”€ fact_sales/
```

## ğŸ› ï¸ Customization

### Adding New Data Sources

1. Create a new source class inheriting from `DataSource`
2. Implement `extract()` and `validate()` methods
3. Add configuration to `pipeline_config.yaml`

### Adding New Transformations

1. Create transformation functions in `transformation/` directory
2. Update the Airflow DAG to include new tasks
3. Test transformations with sample data

## ğŸ“¦ Dependencies

Major dependencies:
- **PySpark**: Distributed data processing
- **Apache Airflow**: Workflow orchestration
- **Boto3**: AWS SDK for Python
- **Pandas**: Data manipulation
- **SQLAlchemy**: Database connectivity

See `requirements.txt` for complete list.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ‘¥ Authors

- Your Team Name

## ğŸ“ Support

For issues and questions:
- Create an issue in the GitHub repository
- Contact the data engineering team

## ğŸ”® Future Enhancements

- [ ] Add support for real-time streaming with Kafka
- [ ] Implement data lineage tracking
- [ ] Add data quality monitoring with Great Expectations
- [ ] Support for additional data sources (MongoDB, Snowflake)
- [ ] Machine learning model integration
- [ ] Advanced data profiling and cataloging